import os
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from dotenv import load_dotenv
load_dotenv()

# --- Config ---
CSV_PATH = "data/train.csv"       # file c·ªßa b·∫°n
FAISS_FOLDER = "vectors/faiss_index"

# --- API Key (ch·ªâ d√πng 1 key) ---
API_KEY = os.getenv("GOOGLE_API_KEY_1")

# --- Load data ---
df = pd.read_csv(CSV_PATH)
texts = df['INFOR'].fillna("").astype(str).tolist()
metadatas = [{"id": int(row["ID"]), "choice": int(row["CHOICE"])} for _, row in df.iterrows()]

# --- Kh·ªüi t·∫°o model embeddings d√πng 1 key ---
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/gemini-embedding-001",
    google_api_key=API_KEY
)

# --- Build FAISS in batches (kh√¥ng gi·ªõi h·∫°n rate) ---
def build_faiss_in_batches(texts, metadatas, embeddings_model, faiss_folder, batch_size=20):
    db = None
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        batch_metas = metadatas[i:i+batch_size]
        try:
            batch_db = FAISS.from_texts(
                texts=batch_texts,
                embedding=embeddings_model,
                metadatas=batch_metas
            )
            if db is None:
                db = batch_db
            else:
                db.merge_from(batch_db)
            print(f"‚úÖ Processed {i+len(batch_texts)}/{len(texts)} rows")
        except Exception as e:
            print(f"‚ùå L·ªói t·∫°i batch {i//batch_size + 1}: {str(e)}")
            continue
    if db is not None:
        db.save_local(faiss_folder)
        print(f"‚úÖ FAISS index saved to '{faiss_folder}'")
    else:
        print("‚ùå Kh√¥ng th·ªÉ t·∫°o FAISS index")
    return db

# --- Run ---
if __name__ == "__main__":
    print(f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(texts)} texts v·ªõi 1 API key")
    db = build_faiss_in_batches(texts, metadatas, embeddings, FAISS_FOLDER, batch_size=10)
